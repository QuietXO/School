{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **_PyTorch Basics_**\n",
    "\n",
    "#### _https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Call the Libraries & GPU*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchvision  # 1st use: Dataset & Dataloader\n",
    "import torch.nn as nn  # 1st use: Training Pipeline\n",
    "import torch.nn.functional as F  # 1st use: Activation Functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets  # 1st use: Linear Regression\n",
    "from sklearn.preprocessing import StandardScaler  # 1st use: Logistic Regression\n",
    "from sklearn.model_selection import train_test_split  # 1st use: Logistic Regression\n",
    "from torch.utils.data import Dataset, DataLoader  # 1st use: Dataset & Dataloader\n",
    "\n",
    "# Device config (Pick your set-up)\n",
    "GPU = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # NVIDIA GPUs\n",
    "# GPU = torch.device('mps' if torch.has_mps else 'cpu')  # ARM GPUs (M1, M2, ...)\n",
    "print('Using the Processor') if GPU == torch.device('cpu') else print('Using the Graphics Card')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test the GPU (mainly for ARM GPU users)\n",
    "dtype = torch.float32\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=GPU, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=GPU, dtype=dtype)\n",
    "b = torch.randn((), device=GPU, dtype=dtype)\n",
    "c = torch.randn((), device=GPU, dtype=dtype)\n",
    "d = torch.randn((), device=GPU, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 500 == 0:\n",
    "        print(f'Epoch: {t} | Loss: {loss:.4f}')\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item():.4f} + {b.item():.4f}x + '\n",
    "      f'{c.item():.4f}x^2 + {d.item():.4f}x^3')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Create a Tensor and basic operations*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 1],[1, 1]], dtype=torch.float64)\n",
    "print(x)\n",
    "print(x.dtype)\n",
    "y = torch.rand(2, 2)\n",
    "print(y)\n",
    "print(f'{y.dtype}\\n')\n",
    "# z = x + y\n",
    "# print(z)\n",
    "\n",
    "# y = y.add(x)\n",
    "# y = torch.add(x, y) # add(+), sub(-), mul(*), div(/)\n",
    "y.add_(x)  # in-place operation (same as line 10)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Select and view certain stuff*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.rand(5, 4)\n",
    "print(x, x.size())\n",
    "print(x[:, 0])  # print 1st column\n",
    "print(x[1, :])  # print 2nd row\n",
    "print(x[0, 0])\n",
    "print(x[0, 0].item())  # print a direct value\n",
    "\n",
    "y = x.view(5*4)  # re-shapes the tensor into 1D\n",
    "print(y, y.size())\n",
    "\n",
    "y = x.view(-1, 10)  # calculate the other value\n",
    "print(y, y.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Convert from torch to numpy and vice-versa*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.rand(5)\n",
    "print(x, type(x))\n",
    "y = x.numpy()  # y shares the location with x, changing x will change y as well\n",
    "print(f'{y} {type(y)}\\n')\n",
    "\n",
    "x = np.ones(5)\n",
    "print(x, type(x))\n",
    "y = torch.from_numpy(x)  # y shares the location with x, changing x will change y as well\n",
    "print(y, type(y))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Create a Tensor on the GPU*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.rand(5, device=GPU)  # cannot be converted to numpy\n",
    "y = torch.rand(5)\n",
    "y = y.to(GPU)\n",
    "z = x + y\n",
    "z = z.to(\"cpu\")\n",
    "\n",
    "try:\n",
    "    x = x.numpy()\n",
    "except TypeError:\n",
    "    print(\"Cannot covert to GPU Tensor Numpy\")\n",
    "\n",
    "try:\n",
    "    z = z.numpy()\n",
    "    print(z, type(z))\n",
    "except TypeError:\n",
    "    print(\"Cannot covert to GPU Tensor Numpy\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*AutoGrad*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "# ⌄⌄⌄ These methods will remove grad\n",
    "# x.requires_grad_(False)\n",
    "# x.detach_()\n",
    "# with torch.no_grad():\n",
    "#   y = x + 2\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "z = y * y * 2\n",
    "z = z.mean()\n",
    "print(z)\n",
    "\n",
    "z.backward()  # dz/dx\n",
    "print(x.grad)\n",
    "\n",
    "# Dummy Function\n",
    "weights = torch.randn(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights * 3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "\n",
    "    weights.grad.zero_()  # we have to clear the grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Backpropagation*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass and compute the loss\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2\n",
    "print(loss)\n",
    "\n",
    "# backward pass\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "### update weights\n",
    "### next forward and backward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Manual Gradient Descent*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y) ** 2).mean()\n",
    "\n",
    "\n",
    "# gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2x * (w*x - y)\n",
    "def gradiant(x, y, y_predicted):\n",
    "    return np.dot(2 * x, y_predicted - y).mean()\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    L = loss(Y, y_pred)\n",
    "\n",
    "    # gradients\n",
    "    dw = gradiant(X, Y, y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {L:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Automatic Gradient Descent*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y) ** 2).mean()\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 80\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    L = loss(Y, y_pred)\n",
    "\n",
    "    # gradients = backward pass\n",
    "    L.backward()  # dL/dw\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    # !!! ZERO the gradients !!!\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % int(n_iters/10) == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {L:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Training Pipeline (Manual Prediction)*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1.) Design model (input, output size, forward pass)\n",
    "# 2.) Construct loss and optimizer\n",
    "# 3.) Training loop\n",
    "#       - forward pass: compute prediction\n",
    "#       - backward pass: gradients\n",
    "#       - update weights\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 80\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    L = loss(Y, y_pred)\n",
    "\n",
    "    # gradients = backward pass\n",
    "    L.backward()  # dL/dw\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # !!! ZERO the gradients !!!\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % int(n_iters/10) == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {L:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Training Pipeline (Automatic Prediction)*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1.) Design model (input, output size, forward pass)\n",
    "# 2.) Construct loss and optimizer\n",
    "# 3.) Training loop\n",
    "#       - forward pass: compute prediction\n",
    "#       - backward pass: gradients\n",
    "#       - update weights\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) #, device=GPU)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32) #, device=GPU)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32) #, device=GPU)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "\n",
    "# model = nn.Linear(input_size, output_size) #, device=GPU)\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.001\n",
    "n_iters = 30000\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # loss\n",
    "    L = loss(Y, y_pred)\n",
    "\n",
    "    # gradients = backward pass\n",
    "    L.backward()  # dL/dw\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # !!! ZERO the gradients !!!\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % int(n_iters/10) == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {L:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Linear Regression*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 0.) Data Preparation\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# 1.) Model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2.) Loss & Optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3.) Training Loop\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward Pass & Loss\n",
    "    y_predicted = model(X)\n",
    "    L = criterion(y_predicted, y)\n",
    "    # Backward Pass\n",
    "    L.backward()\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "    # !!! Zero Gradients !!!\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % int(num_epochs/10) == 0:\n",
    "        print(f'epoch {epoch + 1}: loss = {L.item():.8f}')\n",
    "\n",
    "# Plot\n",
    "predicted = model(X).detach().numpy()\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Logistic Regression*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 0.) Data Preparation\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Scale\n",
    "sc = StandardScaler()  # recommended with Logistic Regression\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "# X_train = X_train.to(GPU)\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "# X_test = X_test.to(GPU)\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "# y_train = y_train.to(GPU)\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "# y_test = y_test.to(GPU)\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "\n",
    "# 1.) Model\n",
    "# f = wx + b, Sigmoid at the End\n",
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "# model = model.to(GPU)\n",
    "\n",
    "# 2.) Loss & Optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3.) Training Loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward Pass & Loss\n",
    "    yPredicted = model(X_train)\n",
    "    L = criterion(yPredicted, y_train)\n",
    "    # Backward Pass\n",
    "    L.backward()\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "    # !!! Zero Gradients !!!\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % int(num_epochs/10) == 0:\n",
    "        print(f'epoch {epoch + 1}: loss = {L.item():.8f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    yPredicted = model(X_test)\n",
    "    yPredicted_cls = yPredicted.round()\n",
    "    acc = yPredicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'Accuracy = {acc:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Dataset & Dataloader*__\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "__epoch = 1 forward & backward pass of ALL training samples__\n",
    "\n",
    "__batch_size = number of training samples in one forward & backward pass__\n",
    "\n",
    "__number of iterations = number of passes, each pass using [batch_size] number of samples__\n",
    "\n",
    "__e.g. 100 samples, batch_size=20 --> 100/20 = 5 iterations per 1 epoch?__\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Load Data\n",
    "        xy = np.loadtxt('./datasets/wine.csv', delimiter=',',dtype=np.float32, skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[:, 1:])\n",
    "        self.y = torch.from_numpy(xy[:, [0]])  # n_samples, 1\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "WineDataset = WineDataset()\n",
    "\n",
    "first_data = WineDataset[0]\n",
    "features, labels = first_data\n",
    "print(features, labels)\n",
    "\n",
    "\n",
    "# !!! IF YOU GET AN ERROR DURING LOADING, SET num_workers TO 0 !!!\n",
    "train_loader = DataLoader(dataset=WineDataset,  # Load whole dataset with DataLoader\n",
    "                          batch_size=4,\n",
    "                          shuffle=True,  # shuffle: shuffle data, good for training\n",
    "                          num_workers=0)  # num_workers: faster loading with multiple subprocesses\n",
    "\n",
    "# convert to an iterator and look at one random sample\n",
    "dataiter = iter(train_loader)\n",
    "data = next(dataiter)\n",
    "features, labels = data\n",
    "print(features, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Dataset Transforms*__\n",
    "\n",
    "_https://pytorch.org/vision/stable/transforms.html_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        # Load Data\n",
    "        xy = np.loadtxt('D:./datasets/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "        self.x = xy[:, 1:]\n",
    "        self.y = xy[:, [0]]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x[index], self.y[index]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "\n",
    "\n",
    "class MulTransform:\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        inputs *= self.factor\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "WineDatasetNumpy = WineDataset(transform=None)\n",
    "first_data = WineDatasetNumpy[0]\n",
    "features, labels = first_data\n",
    "print(type(features), type(labels))\n",
    "print(f'{features}\\n')\n",
    "\n",
    "WineDatasetTensor = WineDataset(transform=ToTensor())\n",
    "first_data = WineDatasetTensor[0]\n",
    "features, labels = first_data\n",
    "print(type(features), type(labels))\n",
    "print(f'{features}\\n')\n",
    "\n",
    "# Combine more transformations\n",
    "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(2)])\n",
    "WineDatasetComposed = WineDataset(transform=composed)\n",
    "first_data = WineDatasetComposed[0]\n",
    "features, labels = first_data\n",
    "print(type(features), type(labels))\n",
    "print(features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Softmax*__\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "__Softmax applies the exponential function to each element, and normalizes__\n",
    "__by dividing by the sum of all these exponentials__\n",
    "__-> squashes the output to be between 0 and 1 = probability__\n",
    "__sum of all probabilities is 1__\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "outputs = softmax(x)\n",
    "print(f'Softmax Numpy: {outputs}')\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0)\n",
    "print(f'Softmax Torch: {outputs}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Cross-Entropy*__\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "__Cross-entropy loss, or log loss,__\n",
    "__measures the performance of a classification model__\n",
    "__whose output is a probability value between 0 and 1.__\n",
    "__-> loss increases as the predicted probability diverges from the actual label__\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "         -> 2.0              -> 0.65\n",
    "  Linear -> 1.0  -> Softmax  -> 0.25   -> CrossEntropy(y, y_hat)\n",
    "         -> 0.1              -> 0.1\n",
    "\n",
    "      scores(logits)      probabilities\n",
    "                            sum = 1.0\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Numpy Cross-Entropy\n",
    "def cross_entropy(actual, predicted):\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss  # / float(predicted.shape[0])\n",
    "\n",
    "\n",
    "# y must be one hot encoded\n",
    "# if class 0: [1 0 0]\n",
    "# if class 1: [0 1 0]\n",
    "# if class 2: [0 0 1]\n",
    "Y = np.array([1, 0, 0])\n",
    "\n",
    "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
    "L1 = cross_entropy(Y, Y_pred_good)\n",
    "L2 = cross_entropy(Y, Y_pred_bad)\n",
    "print(f'Loss1 Numpy: {L1:.4f}')\n",
    "print(f'Loss2 Numpy: {L2:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "# Torch Cross-Entropy\n",
    "L = nn.CrossEntropyLoss()\n",
    "'''\n",
    "Y = torch.tensor([0]) # 1 samples\n",
    "# n_samples x n_classes = 1x3\n",
    "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])\n",
    "'''\n",
    "Y = torch.tensor([2, 0, 1])  # 3 samples\n",
    "# n_samples x n_classes = 3x3\n",
    "Y_pred_good = torch.tensor([[0.2, 1.0, 2.1], [2.0, 1.0, 0.1], [1.0, 3.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3], [0.5, 1.0, 2.1], [2.0, 1.0, 0.1]])\n",
    "\n",
    "L1 = L(Y_pred_good, Y)\n",
    "L2 = L(Y_pred_bad, Y)\n",
    "\n",
    "print(f'Loss1 Torch: {L1.item():.4f}')\n",
    "print(f'Loss2 Torch: {L2.item():.4f}')\n",
    "\n",
    "_, predictions1 = torch.max(Y_pred_good, 1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
    "print(f'Prediction1: {predictions1}')\n",
    "print(f'Prediction2: {predictions2}')\n",
    "\n",
    "\n",
    "\n",
    "# Binary Classification Dummy Function\n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = NeuralNet1(input_size=28 * 28, hidden_size=5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "# Multiclass Problem Dummy Function\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # no softmax at the end\n",
    "        return out\n",
    "\n",
    "\n",
    "model = NeuralNet2(input_size=28 * 28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()  # (applies Softmax)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Activation Functions*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.tensor([-1.0, 1.0, 2.0, 3.0])\n",
    "\n",
    "# Softmax\n",
    "output = torch.softmax(x, dim=0)\n",
    "print(f'Softmax: {output}')\n",
    "sm = nn.Softmax(dim=0)\n",
    "output = sm(x)\n",
    "print(f'Softmax: {output}')\n",
    "\n",
    "# Sigmoid\n",
    "output = torch.sigmoid(x)\n",
    "print(f'Sigmoid: {output}')\n",
    "s = nn.Sigmoid()\n",
    "output = s(x)\n",
    "print(f'Sigmoid: {output}')\n",
    "\n",
    "# Tanh\n",
    "output = torch.tanh(x)\n",
    "print(f'Tanh: {output}')\n",
    "t = nn.Tanh()\n",
    "output = t(x)\n",
    "print(f'Tanh: {output}')\n",
    "\n",
    "# ReLU\n",
    "output = torch.relu(x)\n",
    "print(f'ReLU: {output}')\n",
    "relu = nn.ReLU()\n",
    "output = relu(x)\n",
    "print(f'ReLU: {output}')\n",
    "\n",
    "# Leaky ReLU\n",
    "output = F.leaky_relu(x)\n",
    "print(f'LeakyReLU: {output}')\n",
    "l_relu = nn.LeakyReLU()\n",
    "output = l_relu(x)\n",
    "print(f'LeakyReLU: {output}')\n",
    "\n",
    "# nn.ReLU() creates a nn.Module which you can add e.g. to a nn.Sequential model.\n",
    "# torch.relu on the other side is just the functional API call to the relu function,\n",
    "# so that you can add it e.g. in your forward method yourself.\n",
    "\n",
    "\n",
    "# option 1 (create nn modules)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# option 2 (use activation functions directly in forward pass)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Feed-Forward Neural Net*__\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 784  # 28*28 image\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./datasets', train=True,\n",
    "                                           transform=torchvision.transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./datasets', train=False,\n",
    "                                          transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Check the size\n",
    "examples = iter(train_loader)\n",
    "samples, labels = next(examples)\n",
    "#print(samples.shape, labels.shape)\n",
    "\n",
    "# Show the images\n",
    "#for i in range(6):\n",
    "#    plt.subplot(2, 3, i + 1)\n",
    "#    plt.imshow(samples[i][0], cmap='gray')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(GPU)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Images = 100, 1, 28, 28\n",
    "        # We need: 100, 784\n",
    "        #images = images.reshape(-1, samples.shape[2]*samples.shape[3]).to(GPU)\n",
    "        images = images.reshape(-1, 28*28).to(GPU)\n",
    "        labels = labels.to(GPU)\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = model(images)  # Get images\n",
    "        loss = criterion(outputs, labels)  # Predicted outputs\n",
    "\n",
    "        # Backward Pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print some information\n",
    "        n_steps = 10  # Number of print-outs\n",
    "        if (i + 1) % int(1000/n_steps) == 0:\n",
    "            print(f'Epoch {epoch + 1} / {num_epochs} | Step {i+1} / {n_total_steps} | '\n",
    "                  f'Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(GPU)\n",
    "        labels = labels.to(GPU)\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Value, Index\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the model: {accuracy}%')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### __*Convolutional Neural Net (CNN)*__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dataset has PILImage images of range [0, 1].\n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./datasets', train=True,\n",
    "                                             transform=transform, download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./datasets', train=False,\n",
    "                                            transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # un-normalize\n",
    "    np_img = img.numpy()\n",
    "    plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "#imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # input = 3 colour channels, output = 6 (our choice), filter = 5*5 (our choice)\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # take 2*2, then move 2 px\n",
    "        # input = 6 channels, output = 16 (our choice), filter = 5*5 (our choice)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ConvNet().to(GPU)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "        images = images.to(GPU)\n",
    "        labels = labels.to(GPU)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward & optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print(f'Epoch {epoch + 1} / {num_epochs} | Step {i+1} / {n_total_steps} | '\n",
    "                  f'Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(GPU)\n",
    "        labels = labels.to(GPU)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if label == pred:\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    accuracy = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the Model: {accuracy} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Want to learn more about CNNs?__ <br>\n",
    "__[Stanford Lecture (Video)](https://youtu.be/bNb2fEVKeEo)__ <br>\n",
    "__[deeplizard (Video)](https://youtu.be/YRhxdVk_sIs)__ <br>\n",
    "__[GitHub (Article)](https://cs231n.github.io/convolutional-networks/)__ <br>\n",
    "__[Machine Learning Mastery (Article)](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/)__ <br> <br>"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
